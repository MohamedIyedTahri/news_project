{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6795ec54",
   "metadata": {},
   "source": [
    "# Week 4 (Expanded, Clean Version): Data Preparation & Embedding Comparison\n",
    "\n",
    "This clean notebook consolidates the full Week 4 workflow using the expanded News dataset: data preparation, corpus statistics, and embedding comparison (CBOW vs Skip-gram vs BERT). It is self-healing: you can run cells out of order and helper functions will (re)build required artifacts.\n",
    "\n",
    "**Objectives**\n",
    "1. Load and clean the news dataset (SQLite).\n",
    "2. Explore basic corpus statistics (lengths, tokens, categories).\n",
    "3. Prepare tokenized corpus for embeddings.\n",
    "4. Train Word2Vec (CBOW & Skip-gram) and inspect vocab/neighbors.\n",
    "5. Generate contextual BERT (DistilBERT) sentence embeddings (GPU-aware).\n",
    "6. Compare sentence-level similarity (Word2Vec vs BERT).\n",
    "7. Summarize performance & qualitative differences.\n",
    "8. Provide clear conclusions & next steps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b51f7c",
   "metadata": {},
   "source": [
    "## 1. Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Uncomment if running in a fresh environment\n",
    "# !pip install pandas numpy matplotlib seaborn gensim nltk transformers torch tqdm scikit-learn --quiet\n",
    "\n",
    "import sqlite3, time, random, os, re, string, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Ensure NLTK resources\n",
    "for pkg in ['punkt','stopwords']:\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt') if pkg=='punkt' else nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download(pkg, quiet=True)\n",
    "# punkt_tab (newer nltk)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab/english')\n",
    "except LookupError:\n",
    "    try: nltk.download('punkt_tab', quiet=True)\n",
    "    except Exception: pass\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "sns.set_theme(style='whitegrid')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('CUDA available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2ac86",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c1ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = '../news_articles.db'  # Adjust if needed\n",
    "MIN_CONTENT_LEN = 50\n",
    "TOKEN_MIN_LEN = 3\n",
    "BERT_MODEL_NAME = 'distilbert-base-uncased'\n",
    "BERT_MAX_LEN = 128\n",
    "BERT_BATCH_SIZE = 24\n",
    "W2V_PARAMS = dict(vector_size=100, window=5, min_count=3, workers=4, epochs=10, seed=SEED)\n",
    "SENTENCE_SAMPLE_SIZE = 160  # for BERT and similarity comparisons (auto-clamped to corpus size)\n",
    "\n",
    "assert Path(DB_PATH).exists(), f'Database not found at {DB_PATH}'\n",
    "print('Configuration OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1abdec1",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    df = pd.read_sql_query(\n",
    "        'SELECT id, title, content, category, source, publish_date, full_content FROM articles ORDER BY id ASC', conn\n",
    "    )\n",
    "print('Raw shape:', df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412bfeae",
   "metadata": {},
   "source": [
    "## 4. Cleaning & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8073761",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "PUNCT_TABLE = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    text = (text or '').lower()\n",
    "    text = text.translate(PUNCT_TABLE)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return [t for t in word_tokenize(text) if t not in STOPWORDS and len(t) >= TOKEN_MIN_LEN]\n",
    "\n",
    "# Keep only rows with sufficient content length (after fill)\n",
    "df['content'] = df['content'].fillna('')\n",
    "df = df[df['content'].str.len() > MIN_CONTENT_LEN].copy()\n",
    "df['text'] = (df['title'].fillna('') + '. ' + df['content']).str.strip()\n",
    "df['clean_text'] = df['text'].apply(basic_clean)\n",
    "df['tokens'] = df['clean_text'].apply(tokenize)\n",
    "df['token_count'] = df['tokens'].apply(len)\n",
    "print('After cleaning:', df.shape)\n",
    "df[['id','category','token_count']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e19d03",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
    "df['category'].fillna('unknown').value_counts().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Articles per Category')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[1].hist(df['token_count'], bins=30, color='teal', alpha=0.75)\n",
    "axes[1].set_title('Token Count Distribution')\n",
    "axes[1].set_xlabel('Tokens/article')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Total docs:', len(df), 'Mean tokens:', round(df['token_count'].mean(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c50d73",
   "metadata": {},
   "source": [
    "## 6. Corpus & Vocabulary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02467767",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['tokens'].tolist()\n",
    "token_ctr = Counter([t for sent in corpus for t in sent])\n",
    "vocab_size = len(token_ctr)\n",
    "total_tokens = sum(token_ctr.values())\n",
    "ttr = vocab_size / total_tokens if total_tokens else 0\n",
    "print(f'Vocabulary size: {vocab_size} | Total tokens: {total_tokens} | TTR: {ttr:.3f}')\n",
    "pd.DataFrame(token_ctr.most_common(20), columns=['token','freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaccbd32",
   "metadata": {},
   "source": [
    "## 7. Train Word2Vec (CBOW & Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(corpus, sg: int, params: dict):\n",
    "    start = time.time()\n",
    "    model = Word2Vec(sentences=corpus, sg=sg, **params)\n",
    "    elapsed = time.time() - start\n",
    "    return model, elapsed\n",
    "\n",
    "cbow_model, cbow_time = train_word2vec(corpus, sg=0, params=W2V_PARAMS)\n",
    "skip_model, skip_time = train_word2vec(corpus, sg=1, params=W2V_PARAMS)\n",
    "w2v_metrics = {\n",
    "    'cbow': {'vocab': len(cbow_model.wv), 'time_sec': cbow_time},\n",
    "    'skipgram': {'vocab': len(skip_model.wv), 'time_sec': skip_time}\n",
    "} \n",
    "print(f\n",
    ")\n",
    "w2v_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163976fc",
   "metadata": {},
   "source": [
    "## 8. Nearest Neighbors Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_candidates = ['market','technology','government','bbc','investors','bank','china','israel','gaza','economy']\n",
    "anchors = [w for w in anchor_candidates if w in cbow_model.wv] or list(cbow_model.wv.index_to_key[:5])\n",
    "rows = []\n",
    "for w in anchors:\n",
    "    rows.append({\n",
    "        'word': w,\n",
    "        'cbow_neighbors': cbow_model.wv.most_similar(w, topn=5),\n",
    "        'skip_neighbors': skip_model.wv.most_similar(w, topn=5)\n",
    "    })\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f80b84",
   "metadata": {},
   "source": [
    "## 9. PCA Visualization (CBOW vs Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9731b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_sorted = [w for w,_ in token_ctr.most_common(200) if w in cbow_model.wv]\n",
    "VOCAB_SAMPLE = list(dict.fromkeys(anchors + freq_sorted[:60]))\n",
    "pca = PCA(n_components=2, random_state=SEED)\n",
    "cbow_vecs = np.vstack([cbow_model.wv[w] for w in VOCAB_SAMPLE])\n",
    "skip_vecs = np.vstack([skip_model.wv[w] for w in VOCAB_SAMPLE])\n",
    "proj_cbow = pca.fit_transform(cbow_vecs)\n",
    "proj_skip = pca.fit_transform(skip_vecs)\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,6))\n",
    "for ax, proj, title in zip(axes, [proj_cbow, proj_skip], ['CBOW','Skip-gram']):\n",
    "    ax.scatter(proj[:,0], proj[:,1], s=25, alpha=0.75)\n",
    "    for (x,y), w in zip(proj, VOCAB_SAMPLE):\n",
    "        ax.text(x+0.01, y+0.01, w, fontsize=7)\n",
    "    ax.set_title(f'{title} PCA Projection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe698f",
   "metadata": {},
   "source": [
    "## 10. BERT (DistilBERT) Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_bert(sample_size: int = SENTENCE_SAMPLE_SIZE, force: bool=False):\n",
    "    global bert_sample_df, bert_embeddings, tokenizer, bert_model, bert_metrics\n",
    "    if 'bert_embeddings' in globals() and not force:\n",
    "        return bert_embeddings\n",
    "    sample_size = min(sample_size, len(df))\n",
    "    bert_sample_df = df.sample(sample_size, random_state=SEED).reset_index(drop=True)\n",
    "    if 'tokenizer' not in globals() or 'bert_model' not in globals():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "        bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME)\n",
    "        if DEVICE.type == 'cuda': bert_model = bert_model.half()\n",
    "        bert_model.to(DEVICE); bert_model.eval()\n",
    "    batch_size = BERT_BATCH_SIZE\n",
    "    emb_list = []\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, sample_size, batch_size):\n",
    "            texts = bert_sample_df['text'].iloc[i:i+batch_size].tolist()\n",
    "            enc = tokenizer(texts, truncation=True, padding='max_length', max_length=BERT_MAX_LEN, return_tensors='pt')\n",
    "            enc = {k: v.to(DEVICE) for k,v in enc.items()}\n",
    "            with torch.cuda.amp.autocast(enabled=(DEVICE.type=='cuda')):\n",
    "                out = bert_model(**enc).last_hidden_state[:,0,:]  # CLS\n",
    "            if out.dtype == torch.float16: out = out.float()\n",
    "            emb_list.append(out.cpu())\n",
    "    bert_embeddings = torch.vstack(emb_list).numpy()\n",
    "    elapsed = time.time() - start\n",
    "    bert_metrics = { 'sample_size': sample_size, 'encode_time_sec': elapsed, 'avg_time_per_doc_ms': (elapsed*1000)/sample_size }\n",
    "    print(f'BERT embeddings shape: {bert_embeddings.shape} | time={elapsed:.2f}s | ms/doc={bert_metrics[\n",
    "]:.2f}')\n",
    "    return bert_embeddings\n",
    "\n",
    "ensure_bert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b810a",
   "metadata": {},
   "source": [
    "## 11. Sentence Embeddings (Word2Vec Averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf01b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_w2v(tokens, model):\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv]\n",
    "    if not vecs: return np.zeros(model.vector_size)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "w2v_sentence_embeddings = np.vstack([average_w2v(toks, cbow_model) for toks in bert_sample_df['tokens']])\n",
    "w2v_sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e66445",
   "metadata": {},
   "source": [
    "## 12. Similarity Comparison (BERT vs Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIR_COUNT = 6\n",
    "rng = random.Random(SEED)\n",
    "pairs = []\n",
    "seen = set()\n",
    "while len(pairs) < PAIR_COUNT:\n",
    "    i = rng.randrange(0, len(bert_sample_df))\n",
    "    j = rng.randrange(0, len(bert_sample_df))\n",
    "    if i==j: continue\n",
    "    key = tuple(sorted((i,j)))\n",
    "    if key in seen: continue\n",
    "    seen.add(key); pairs.append((i,j))\n",
    "\n",
    "rows = []\n",
    "for i,j in pairs:\n",
    "    b_sim = float(cosine_similarity(bert_embeddings[i].reshape(1,-1), bert_embeddings[j].reshape(1,-1))[0][0])\n",
    "    w_sim = float(cosine_similarity(w2v_sentence_embeddings[i].reshape(1,-1), w2v_sentence_embeddings[j].reshape(1,-1))[0][0])\n",
    "    rows.append({\n",
    "        'pair': f'{i}-{j}',\n",
    "        'bert_sim': round(b_sim,4),\n",
    "        'w2v_sim': round(w_sim,4),\n",
    "        'delta': round(b_sim - w_sim,4),\n",
    "        'snippet_i': bert_sample_df.loc[i,'text'][:70]+'...',\n",
    "        'snippet_j': bert_sample_df.loc[j,'text'][:70]+'...'\n",
    "    })\n",
    "sim_df = pd.DataFrame(rows).assign(abs_delta=lambda d: d['delta'].abs()).sort_values('abs_delta', ascending=False).drop(columns='abs_delta')\n",
    "sim_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9989df3",
   "metadata": {},
   "source": [
    "## 13. Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e0664",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_rows = []\n",
    "for m_name, d in w2v_metrics.items():\n",
    "    metrics_rows.append({\n",
    "        'model': f'word2vec_{m_name}',\n",
    "        'vocab_size': d['vocab'],\n",
    "        'train_time_sec': round(d['time_sec'],2),\n",
    "        'inference_docs_per_sec': None\n",
    "    })\n",
    "metrics_rows.append({\n",
    "    'model': 'bert_distilbert_cls',\n",
    "    'vocab_size': None,\n",
    "    'train_time_sec': 0.0,\n",
    "    'inference_docs_per_sec': round(bert_metrics['sample_size']/bert_metrics['encode_time_sec'],2)\n",
    "})\n",
    "pd.DataFrame(metrics_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d9c0e",
   "metadata": {},
   "source": [
    "## 14. Analysis & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b2dc3d",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- **Vocabulary Coverage**: Increased corpus scale stabilizes co-occurrence statistics; fewer spurious neighbors.\n",
    "- **CBOW vs Skip-gram**: CBOW faster; Skip-gram sometimes surfaces rarer geopolitical / financial terms more precisely.\n",
    "- **PCA**: Clusters show topical groupings (markets, regions); some overlap persists due to corpus size and linear projection.\n",
    "- **BERT Advantage**: Consistently higher discrimination in similarity deltas; contextual subword representation handles OOV/proper nouns better.\n",
    "- **Performance**: Word2Vec training sub-second vs BERT inference cost; still acceptable with batching + FP16.\n",
    "\n",
    "### Limitations\n",
    "- Corpus still modest for rich analogical reasoning.\n",
    "- Lowercasing removes case distinctions important for entity types.\n",
    "- Averaging Word2Vec tokens is a weak sentence embedding baseline.\n",
    "\n",
    "### Recommended Next Steps\n",
    "1. Introduce FastText or subword embeddings for improved OOV handling.\n",
    "2. Add Sentence-BERT (all-MiniLM) for stronger sentence embeddings.\n",
    "3. Evaluate on a downstream task (category classification accuracy).\n",
    "4. Persist monthly snapshots for embedding drift analysis.\n",
    "5. Add Heaps' Law / Zipf diagnostics for scaling justification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d86e3",
   "metadata": {},
   "source": [
    "## 15. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6937e08b",
   "metadata": {},
   "source": [
    "| Aspect | CBOW | Skip-gram | BERT |\n",
    "|--------|------|-----------|------|\n",
    "| Training Time | Fastest | Fast | Slow (inference only) |\n",
    "| Rare Word Handling | Weaker | Better | Strong (subword) |\n",
    "| Context Depth | Local window | Local window | Deep bidirectional |\n",
    "| Sentence Quality (avg) | Coarse | Coarse+ | Fine-grained |\n",
    "| Resource Use | Low | Low | Higher (GPU helps) |\n",
    "| Best Use | Quick baseline | Rare tokens | Downstream semantics |\n",
    "\n",
    "**Summary**: Scaling the corpus significantly improves stability of traditional embeddings, but BERT remains superior for nuanced semantic similarity. Traditional models still valuable for lightweight tasks and rapid experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb5526",
   "metadata": {},
   "source": [
    "## 16. (Optional) Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d668f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to persist models/metrics\n",
    "# cbow_model.save('cbow_expanded.model')\n",
    "# skip_model.save('skipgram_expanded.model')\n",
    "# with open('embedding_metrics.json','w') as f:\n",
    "#     import json; json.dump({'w2v': w2v_metrics, 'bert': bert_metrics}, f, indent=2)\n",
    "# print('Models & metrics saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b97684",
   "metadata": {},
   "source": [
    "## 17. Reproducibility & Execution Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b28a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_report():\n",
    "    print('Have corpus:', 'corpus' in globals())\n",
    "    print('Have Word2Vec models:', 'cbow_model' in globals() and 'skip_model' in globals())\n",
    "    print('Have bert_embeddings:', 'bert_embeddings' in globals())\n",
    "    if 'bert_metrics' in globals(): print('BERT ms/doc:', round(bert_metrics['avg_time_per_doc_ms'],2))\n",
    "status_report()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
